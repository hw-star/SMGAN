<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SMGAN-HW</title>
  <link rel="stylesheet" href="./css/index.css">
  <link rel="shortcut icon" href="./images/favicon.ico">
</head>
<body>
  <div class="section centered">
    <h1>‚ú® SMGAN: A Multi-Path Generative Adversarial Network for Remote Sensing Image Super-Resolution Reconstruction</h1>
    <table align="center" width="675px" style="margin-bottom: 1rem;">
		<tr>
		  <td align="center" width="310px">
			<span style="font-size:20px">
			  <a href="https://hw-star.icu/">Wei Huo<sup>1</sup></a>
			</span>
		  </td>
		  <td align="center" width="310px">
			<span style="font-size:20px">
			  <a href="https://cs.qhu.edu.cn/jxgz/jxysz/sssds2/f9308b1a4bee44f09f2aa9833df9dbfc.htm">Xiaodan Zhang<sup>1,2</sup></a>
			</span>
		  </td>
		  <td align="center" width="310px">
			<span style="font-size:20px">Qiyuan Zhang<sup>1</sup></span>
		  </td>
		  <td align="center" width="310px">
			<span style="font-size:20px">Naihao Hu<sup>1</sup></span>
		  </td>
		</tr>
	  </table>
	  
<!-- 	  <table align="center" width="475px" style="margin-bottom: 1rem;">
		<tr>
		  <td align="center" width="150px">
			<span style="font-size:20px">xxxxxx<sup>1</sup></span>
		  </td>
		  <td align="center" width="150px">
			<span style="font-size:20px">xxxxxxx<sup>1</sup></span>
		  </td>
		  <td align="center" width="150px">
			<span style="font-size:20px">xxxxxxx<sup>1</sup></span>
		  </td>
		</tr>
	  </table> -->
	  
	  <table align="center" width="1100px" style="margin-bottom: 1rem;">
		<tr>
		  <td align="center" style="text-align: center;">
			<span style="font-size:18px;">
			  <a href="https://cs.qhu.edu.cn/"><sup>1</sup> School of Computer Technology and Applications, Qinghai University, Xining 810016, China</a>
			</span>
		  </td>
		</tr>
		<tr>
		  <td align="center" style="text-align: center;">
			<span style="font-size:18px;">
			  <a href="https://cs.qhu.edu.cn/sys/sysgk/index.htm"><sup>2</sup>Qinghai Provincial Laboratory for Intelligent Computing and Application, Qinghai University, Xining, Qinghai 810016, China</a>
			</span>
		  </td>
		</tr>
	  </table>
	  
	  <table align="center" width="350px">
		<tr>
		  <td align="center" width="120px">
			<span style="font-size:22px">
			  <a href="">üìù arXiv</a>
			</span>
		  </td>
		  <td align="center" width="120px">
			<span style="font-size:22px">
			  <a href="https://github.com/hw-star/SMGAN">‚öôÔ∏è code</a>
			</span>
			<br>
		  </td>
		</tr>
	  </table>
	  
  </div>

  <div class="section centered">
	<h2>üìÉ ÊëòË¶Å</h2>
  </div>
  <div class="section">
	<div class="abstract">
	  <p>Remote sensing image super-resolution (SR) plays a critical role in compensating for the missing information in high-resolution (HR) imagery. However, traditional methods often struggle to strike a balance between preserving fine-grained local details and maintaining global structural consistency. The effective fusion of multi-scale features remains a challenging issue.<br> &emsp;&emsp; To address these limitations, this paper proposes a novel multi-path generative adversarial network (SMGAN) tailored for remote sensing image super-resolution reconstruction. SMGAN integrates three heterogeneous feature extraction branches‚Äîa deep residual convolutional network, an enhanced Mamba module, and a constant-scale Swin Transformer‚Äîto model image representations at local, regional, and global levels, respectively. This design enables comprehensive characterization of fine details, spatial structures, and contextual semantics. To further enhance the quality of multi-branch feature fusion, we introduce a Residual Attention Module (RAM), which employs a two-stage mechanism to achieve effective coupling and selective enhancement between the main feature stream and the fused stream. Considering the critical importance of edge structures and textural details in remote sensing imagery, we design a dual-discriminator architecture operating in both the image and gradient domains. Additionally, a structure-aware gradient loss function is proposed to better preserve edge sharpness and fine textures during reconstruction.<br> &emsp;&emsp; <strong style="color: #007acc;">Extensive experiments conducted on the self-built high-resolution remote sensing SR dataset RS-SR19 and the public land-use classification dataset AID demonstrate that SMGAN surpasses various state-of-the-art SR methods in terms of traditional quantitative metrics (e.g., PSNR and SSIM) as well as subjective visual quality. Notably, the model achieves mean LPIPS scores of approximately 0.344 and 0.357 on RS-SR19 and AID, respectively, indicating superior perceptual fidelity and detail restoration. Furthermore, on real-world remote sensing data from the complex terrain of the Sanjiangyuan region in Qinghai, SMGAN exhibits remarkable structural consistency and textural continuity, with robust performance under cloud occlusion conditions and a peak PSNR of around 36dB, highlighting its strong generalization and resilience.</strong></p>
	</div>
  </div>
  <hr class="section-divider">

  <div class="section centered">
    <h2>‰∏Ä„ÄÅ Overall flowchart of the model</h2>
    <div class="image-container">
      <img src="./images/liuchengtu.png" alt="Ê®°ÂûãÊï¥‰ΩìÊµÅÁ®ãÂõæ">
    </div>
	<div class="module-description">
		<p>This flowchart shows the overall architecture of the SMGAN model from the input of low-resolution images to the output of high-resolution images.</p>
	</div>
  </div>
  <hr class="section-divider">

  <div class="section centered">
    <h2>‰∫å„ÄÅ Generator Structure Diagram (GAN)</h2>
    <div class="image-container">
      <img src="./images/Êû∂ÊûÑÂõæ.png" alt="ÁîüÊàêÂô®ÁªìÊûÑÂõæ">
	  <div class="module-description">
		<p>The generator structure of SMGAN can be divided into three parts as a whole. The first part is the three-branch feature extraction module. It models the image features of the input image and its gradient map from the local (DRSE) - regional (CSSM) - global (M4X) levels respectively, complementing each other and effectively enhancing the overall quality of image reconstruction. The second part is the two-stage residual enhancement module, which realizes the feature interaction among different modalities and scales with the help of the RAM mechanism to improve the consistency and discriminability of the representation. The third part is the feature fusion and image reconstruction module. Firstly, multi-branch information is aggregated through MERGE, and then the final high-resolution image is generated through ERT.</p>
	  </div>
    </div>
	<div class="image-container">
		<img src="./images/san.png" alt="ÁîüÊàêÂô®ÁªìÊûÑÂõæ">
		<div class="module-description">
			<p>The three modules of the first part.</p>
		</div>
	</div>
	<div class="image-container">
		<img src="./images/ram.png" alt="ÁîüÊàêÂô®ÁªìÊûÑÂõæ">
		<div class="module-description">
			<p>RAM.</p>
		</div>
	</div>
	<div class="image-container">
		<img src="./images/last.png" alt="ÁîüÊàêÂô®ÁªìÊûÑÂõæ">
		<img src="./images/chong.png" alt="ÁîüÊàêÂô®ÁªìÊûÑÂõæ">
		<div class="module-description">
			<p>The third part and pixel rearrangement.</p>
		</div>
	</div>
  </div>
  <hr class="section-divider">

  <div class="section centered">
    <h2>‰∏â„ÄÅ Compare the model data table diagram</h2>
    <div class="image-container">
      <img src="./images/duibi-1.png" alt="ÂØπÊØîÊ®°ÂûãÊï∞ÊçÆ">
    </div>
	<div class="image-container">
		<img src="./images/duibi-2.png" alt="ÂØπÊØîÊ®°ÂûãÊï∞ÊçÆ">
	</div>
	<div class="module-description">
		<p>This paper adopts two datasets for experiments: one is a self-constructed high-quality remote sensing image dataset, named RS-SR19; Another one is the widely used public remote sensing scene classification Dataset AID (Aerial Image Dataset). Five mainstream evaluation indicators were adopted: PSNR, SSIM, LPIPS, RMSE and SAM to evaluate the model performance from multiple dimensions such as image reconstruction accuracy, structural consistency, perceptual quality and spectral retention ability respectively.</p>
	</div>
  </div>
  <hr class="section-divider">

  <div class="section centered">
    <h2>Âõõ„ÄÅ Visual image display</h2>
    <div class="image-container">
      <img src="./images/ke-1.png" alt="ÂèØËßÜÂåñÂõæ1">
	  (a)
      <img src="./images/ke-2.png" alt="ÂèØËßÜÂåñÂõæ2">
	  (b)
	  <img src="./images/ke-3.png" alt="ÂèØËßÜÂåñÂõæ3">
	  (c)
    </div>
	<div class="module-description">
		<p>(a) is the visual comparison of the self-made dataset RS-SR19, (b) is the visual comparison of the public dataset AID, and (c) is the visual effect of the Qinghai Sanjiangyuan dataset.</p>
	</div>
  </div>
  <hr class="section-divider">

  <div class="section centered">
    <h2>‰∫î„ÄÅ Ablation experiment results</h2>
    <div class="image-container">
      <img src="./images/duibi-ram.png" alt="Ê∂àËûçÂÆûÈ™åÁªìÊûúÂõæ">
	  (a)
	  <img src="./images/duibi-ab.png" alt="Ê∂àËûçÂÆûÈ™åÁªìÊûúÂõæ">
	  (b)
	  <img src="./images/duibi-ert.png" alt="Ê∂àËûçÂÆûÈ™åÁªìÊûúÂõæ">
	  (c)
	  <img src="./images/loss.png" alt="Ê∂àËûçÂÆûÈ™åÁªìÊûúÂõæ">
	  (d)
    </div>
	<div class="module-description">
		<p>(a) is the ablation experiment of the residual enhancement module (RAM), (b) is the ablation experiment of the weighted hyperparameters in the loss function, (c) is the ablation experiment of the reconstructed channel output factor, and (d) is the ablation experiment of the specific gradient loss term.</p>
	</div>
  </div>
</body>
</html>








